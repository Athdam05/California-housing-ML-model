{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNifDuPd0MpTJa5dahMe4rf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Athdam05/California-housing-ML-model/blob/main/Housing_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-dVoaV9pE4O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74d7490e"
      },
      "source": [
        "# Task\n",
        "Perform a regression analysis on the \"housing.csv\" dataset. This involves loading the data, handling missing values, encoding categorical features, scaling numerical features, and splitting the data into training and testing sets. Train and evaluate Linear Regression, Ridge Regression, and Decision Tree Regressor models, comparing their performance using RMSE and MAE. Finally, analyze the models for underfitting/overfitting and discuss a real-world machine learning issue observed in the context of this dataset or models, summarizing the findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0dd469e"
      },
      "source": [
        "## Load Data and Initial Inspection\n",
        "\n",
        "### Subtask:\n",
        "Load the `housing.csv` dataset into a pandas DataFrame. Perform an initial inspection to understand its structure, identify data types, and check for any immediate signs of missing values or categorical features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4334713f"
      },
      "source": [
        "**Reasoning**:\n",
        "To address the subtask, I need to load the specified CSV file into a pandas DataFrame, perform initial inspections including displaying the head, checking data types and non-null values with `.info()`, generating descriptive statistics with `.describe()`, and identifying missing values using `.isnull().sum()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b573372a",
        "outputId": "76c2c5b3-1f25-4443-a2f1-120fc8852d7d"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the housing.csv file into a pandas DataFrame\n",
        "df = pd.read_csv('/content/housing.csv')\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "print('First 5 rows of the DataFrame:')\n",
        "print(df.head())\n",
        "print('\\n')\n",
        "\n",
        "# Display the concise summary of the DataFrame\n",
        "print('DataFrame Info:')\n",
        "df.info()\n",
        "print('\\n')\n",
        "\n",
        "# Generate descriptive statistics of the DataFrame\n",
        "print('Descriptive Statistics:')\n",
        "print(df.describe())\n",
        "print('\\n')\n",
        "\n",
        "# Check for the total count of missing values in each column\n",
        "print('Missing values per column:')\n",
        "print(df.isnull().sum())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the DataFrame:\n",
            "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
            "0    -122.23     37.88                41.0        880.0           129.0   \n",
            "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
            "2    -122.24     37.85                52.0       1467.0           190.0   \n",
            "3    -122.25     37.85                52.0       1274.0           235.0   \n",
            "4    -122.25     37.85                52.0       1627.0           280.0   \n",
            "\n",
            "   population  households  median_income  median_house_value ocean_proximity  \n",
            "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
            "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
            "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
            "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
            "4       565.0       259.0         3.8462            342200.0        NEAR BAY  \n",
            "\n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20640 entries, 0 to 20639\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           20640 non-null  float64\n",
            " 1   latitude            20640 non-null  float64\n",
            " 2   housing_median_age  20640 non-null  float64\n",
            " 3   total_rooms         20640 non-null  float64\n",
            " 4   total_bedrooms      20433 non-null  float64\n",
            " 5   population          20640 non-null  float64\n",
            " 6   households          20640 non-null  float64\n",
            " 7   median_income       20640 non-null  float64\n",
            " 8   median_house_value  20640 non-null  float64\n",
            " 9   ocean_proximity     20640 non-null  object \n",
            "dtypes: float64(9), object(1)\n",
            "memory usage: 1.6+ MB\n",
            "\n",
            "\n",
            "Descriptive Statistics:\n",
            "          longitude      latitude  housing_median_age   total_rooms  \\\n",
            "count  20640.000000  20640.000000        20640.000000  20640.000000   \n",
            "mean    -119.569704     35.631861           28.639486   2635.763081   \n",
            "std        2.003532      2.135952           12.585558   2181.615252   \n",
            "min     -124.350000     32.540000            1.000000      2.000000   \n",
            "25%     -121.800000     33.930000           18.000000   1447.750000   \n",
            "50%     -118.490000     34.260000           29.000000   2127.000000   \n",
            "75%     -118.010000     37.710000           37.000000   3148.000000   \n",
            "max     -114.310000     41.950000           52.000000  39320.000000   \n",
            "\n",
            "       total_bedrooms    population    households  median_income  \\\n",
            "count    20433.000000  20640.000000  20640.000000   20640.000000   \n",
            "mean       537.870553   1425.476744    499.539680       3.870671   \n",
            "std        421.385070   1132.462122    382.329753       1.899822   \n",
            "min          1.000000      3.000000      1.000000       0.499900   \n",
            "25%        296.000000    787.000000    280.000000       2.563400   \n",
            "50%        435.000000   1166.000000    409.000000       3.534800   \n",
            "75%        647.000000   1725.000000    605.000000       4.743250   \n",
            "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
            "\n",
            "       median_house_value  \n",
            "count        20640.000000  \n",
            "mean        206855.816909  \n",
            "std         115395.615874  \n",
            "min          14999.000000  \n",
            "25%         119600.000000  \n",
            "50%         179700.000000  \n",
            "75%         264725.000000  \n",
            "max         500001.000000  \n",
            "\n",
            "\n",
            "Missing values per column:\n",
            "longitude               0\n",
            "latitude                0\n",
            "housing_median_age      0\n",
            "total_rooms             0\n",
            "total_bedrooms        207\n",
            "population              0\n",
            "households              0\n",
            "median_income           0\n",
            "median_house_value      0\n",
            "ocean_proximity         0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d12e2a2"
      },
      "source": [
        "## Preprocess Data\n",
        "\n",
        "### Subtask:\n",
        "Handle missing values in the dataset using appropriate imputation strategies (e.g., mean, median, or mode imputation). Encode categorical variables using one-hot encoding. Apply feature scaling (e.g., StandardScaler) to numerical features to ensure all features contribute equally to the model training. Split the preprocessed dataset into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e14eb0d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires handling missing values, separating features and target, encoding categorical variables, scaling numerical features, and splitting the data. I will address all these steps sequentially in a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "187d196c",
        "outputId": "18983780-f2f3-4ec2-e76a-71b0b0854c85"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Fill missing values in 'total_bedrooms' with its median\n",
        "df['total_bedrooms'].fillna(df['total_bedrooms'].median(), inplace=True)\n",
        "print(\"Missing values after imputation:\")\n",
        "print(df.isnull().sum())\n",
        "print('\\n')\n",
        "\n",
        "# 2. Separate target variable and features\n",
        "X = df.drop('median_house_value', axis=1)\n",
        "y = df['median_house_value']\n",
        "\n",
        "# 3. Identify categorical and numerical columns\n",
        "categorical_cols = ['ocean_proximity']\n",
        "numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "# Remove 'ocean_proximity' from numerical_cols if it was mistakenly included, although select_dtypes handles this\n",
        "numerical_cols = [col for col in numerical_cols if col not in categorical_cols]\n",
        "\n",
        "# 4. Apply one-hot encoding to the 'ocean_proximity' column\n",
        "X_categorical = pd.get_dummies(X[categorical_cols], drop_first=True)\n",
        "\n",
        "# Concatenate one-hot encoded features with numerical features\n",
        "X_numerical = X[numerical_cols]\n",
        "X_processed = pd.concat([X_numerical, X_categorical], axis=1)\n",
        "\n",
        "# 5. Initialize and apply StandardScaler to numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_processed[numerical_cols])\n",
        "X_processed[numerical_cols] = X_scaled\n",
        "\n",
        "print(\"First 5 rows of preprocessed features (X_processed):\")\n",
        "print(X_processed.head())\n",
        "print('\\n')\n",
        "\n",
        "# 6. Split the preprocessed dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values after imputation:\n",
            "longitude             0\n",
            "latitude              0\n",
            "housing_median_age    0\n",
            "total_rooms           0\n",
            "total_bedrooms        0\n",
            "population            0\n",
            "households            0\n",
            "median_income         0\n",
            "median_house_value    0\n",
            "ocean_proximity       0\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "First 5 rows of preprocessed features (X_processed):\n",
            "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
            "0  -1.327835  1.052548            0.982143    -0.804819       -0.972476   \n",
            "1  -1.322844  1.043185           -0.607019     2.045890        1.357143   \n",
            "2  -1.332827  1.038503            1.856182    -0.535746       -0.827024   \n",
            "3  -1.337818  1.038503            1.856182    -0.624215       -0.719723   \n",
            "4  -1.337818  1.038503            1.856182    -0.462404       -0.612423   \n",
            "\n",
            "   population  households  median_income  ocean_proximity_INLAND  \\\n",
            "0   -0.974429   -0.977033       2.344766                   False   \n",
            "1    0.861439    1.669961       2.332238                   False   \n",
            "2   -0.820777   -0.843637       1.782699                   False   \n",
            "3   -0.766028   -0.733781       0.932968                   False   \n",
            "4   -0.759847   -0.629157      -0.012881                   False   \n",
            "\n",
            "   ocean_proximity_ISLAND  ocean_proximity_NEAR BAY  \\\n",
            "0                   False                      True   \n",
            "1                   False                      True   \n",
            "2                   False                      True   \n",
            "3                   False                      True   \n",
            "4                   False                      True   \n",
            "\n",
            "   ocean_proximity_NEAR OCEAN  \n",
            "0                       False  \n",
            "1                       False  \n",
            "2                       False  \n",
            "3                       False  \n",
            "4                       False  \n",
            "\n",
            "\n",
            "Shape of X_train: (16512, 12)\n",
            "Shape of X_test: (4128, 12)\n",
            "Shape of y_train: (16512,)\n",
            "Shape of y_test: (4128,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-214778722.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['total_bedrooms'].fillna(df['total_bedrooms'].median(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "987d5fa6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code produced a `FutureWarning` due to the use of `inplace=True` with `fillna()`. To resolve this, I will modify the line to assign the result of `fillna()` back to the column, which is the recommended practice in newer pandas versions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42256d5a",
        "outputId": "6859f737-7a24-4047-aafc-59b60e1c9777"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Fill missing values in 'total_bedrooms' with its median\n",
        "df['total_bedrooms'] = df['total_bedrooms'].fillna(df['total_bedrooms'].median())\n",
        "print(\"Missing values after imputation:\")\n",
        "print(df.isnull().sum())\n",
        "print('\\n')\n",
        "\n",
        "# 2. Separate target variable and features\n",
        "X = df.drop('median_house_value', axis=1)\n",
        "y = df['median_house_value']\n",
        "\n",
        "# 3. Identify categorical and numerical columns\n",
        "categorical_cols = ['ocean_proximity']\n",
        "numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "# Remove 'ocean_proximity' from numerical_cols if it was mistakenly included, although select_dtypes handles this\n",
        "numerical_cols = [col for col in numerical_cols if col not in categorical_cols]\n",
        "\n",
        "# 4. Apply one-hot encoding to the 'ocean_proximity' column\n",
        "X_categorical = pd.get_dummies(X[categorical_cols], drop_first=True)\n",
        "\n",
        "# Concatenate one-hot encoded features with numerical features\n",
        "X_numerical = X[numerical_cols]\n",
        "X_processed = pd.concat([X_numerical, X_categorical], axis=1)\n",
        "\n",
        "# 5. Initialize and apply StandardScaler to numerical features\n",
        "scaler = StandardScaler()\n",
        "X_processed[numerical_cols] = scaler.fit_transform(X_processed[numerical_cols])\n",
        "\n",
        "print(\"First 5 rows of preprocessed features (X_processed):\")\n",
        "print(X_processed.head())\n",
        "print('\\n')\n",
        "\n",
        "# 6. Split the preprocessed dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values after imputation:\n",
            "longitude             0\n",
            "latitude              0\n",
            "housing_median_age    0\n",
            "total_rooms           0\n",
            "total_bedrooms        0\n",
            "population            0\n",
            "households            0\n",
            "median_income         0\n",
            "median_house_value    0\n",
            "ocean_proximity       0\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "First 5 rows of preprocessed features (X_processed):\n",
            "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
            "0  -1.327835  1.052548            0.982143    -0.804819       -0.972476   \n",
            "1  -1.322844  1.043185           -0.607019     2.045890        1.357143   \n",
            "2  -1.332827  1.038503            1.856182    -0.535746       -0.827024   \n",
            "3  -1.337818  1.038503            1.856182    -0.624215       -0.719723   \n",
            "4  -1.337818  1.038503            1.856182    -0.462404       -0.612423   \n",
            "\n",
            "   population  households  median_income  ocean_proximity_INLAND  \\\n",
            "0   -0.974429   -0.977033       2.344766                   False   \n",
            "1    0.861439    1.669961       2.332238                   False   \n",
            "2   -0.820777   -0.843637       1.782699                   False   \n",
            "3   -0.766028   -0.733781       0.932968                   False   \n",
            "4   -0.759847   -0.629157      -0.012881                   False   \n",
            "\n",
            "   ocean_proximity_ISLAND  ocean_proximity_NEAR BAY  \\\n",
            "0                   False                      True   \n",
            "1                   False                      True   \n",
            "2                   False                      True   \n",
            "3                   False                      True   \n",
            "4                   False                      True   \n",
            "\n",
            "   ocean_proximity_NEAR OCEAN  \n",
            "0                       False  \n",
            "1                       False  \n",
            "2                       False  \n",
            "3                       False  \n",
            "4                       False  \n",
            "\n",
            "\n",
            "Shape of X_train: (16512, 12)\n",
            "Shape of X_test: (4128, 12)\n",
            "Shape of y_train: (16512,)\n",
            "Shape of y_test: (4128,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53c49a99"
      },
      "source": [
        "## Train and Evaluate Linear Regression Model\n",
        "\n",
        "### Subtask:\n",
        "Train a Linear Regression model on the preprocessed training data. Evaluate its performance by calculating Root Mean Squared Error (RMSE) on both the training and testing sets, and Mean Absolute Error (MAE) on the testing set. Store these metrics for comparison.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ed25b11"
      },
      "source": [
        "**Reasoning**:\n",
        "To address the subtask, I will first import the required modules for Linear Regression and evaluation metrics. Then, I will instantiate and train the Linear Regression model, make predictions on both training and testing sets, calculate RMSE for both sets and MAE for the testing set, print these metrics, and finally store them in a dictionary for future comparison.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aba94398",
        "outputId": "e6c17619-4605-41db-831f-3a611cc4bb60"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# 1. Instantiate a Linear Regression model\n",
        "linear_reg_model = LinearRegression()\n",
        "\n",
        "# 2. Train the model\n",
        "linear_reg_model.fit(X_train, y_train)\n",
        "print(\"Linear Regression model trained successfully.\\n\")\n",
        "\n",
        "# 3. Make predictions on the training data\n",
        "y_train_pred_lr = linear_reg_model.predict(X_train)\n",
        "\n",
        "# 4. Make predictions on the testing data\n",
        "y_test_pred_lr = linear_reg_model.predict(X_test)\n",
        "\n",
        "# 5. Calculate RMSE for the training set\n",
        "rmse_train_lr = np.sqrt(mean_squared_error(y_train, y_train_pred_lr))\n",
        "\n",
        "# 6. Calculate RMSE for the testing set\n",
        "rmse_test_lr = np.sqrt(mean_squared_error(y_test, y_test_pred_lr))\n",
        "\n",
        "# 7. Calculate MAE for the testing set\n",
        "mae_test_lr = mean_absolute_error(y_test, y_test_pred_lr)\n",
        "\n",
        "# 8. Print the calculated metrics\n",
        "print(f\"Linear Regression - Training RMSE: {rmse_train_lr:.2f}\")\n",
        "print(f\"Linear Regression - Testing RMSE: {rmse_test_lr:.2f}\")\n",
        "print(f\"Linear Regression - Testing MAE: {mae_test_lr:.2f}\\n\")\n",
        "\n",
        "# 9. Store these metrics in a dictionary for later comparison\n",
        "model_performance = {}\n",
        "model_performance['Linear Regression'] = {\n",
        "    'Train RMSE': rmse_train_lr,\n",
        "    'Test RMSE': rmse_test_lr,\n",
        "    'Test MAE': mae_test_lr\n",
        "}\n",
        "\n",
        "print(\"Metrics stored in 'model_performance' dictionary:\")\n",
        "print(model_performance)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression model trained successfully.\n",
            "\n",
            "Linear Regression - Training RMSE: 68433.94\n",
            "Linear Regression - Testing RMSE: 70060.52\n",
            "Linear Regression - Testing MAE: 50670.74\n",
            "\n",
            "Metrics stored in 'model_performance' dictionary:\n",
            "{'Linear Regression': {'Train RMSE': np.float64(68433.93736666226), 'Test RMSE': np.float64(70060.52184473521), 'Test MAE': 50670.73824097188}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1d0ac5e"
      },
      "source": [
        "## Train and Evaluate Ridge Regression Model\n",
        "\n",
        "### Subtask:\n",
        "Train a Ridge Regression model on the preprocessed training data, experimenting with a suitable alpha value. Evaluate its performance using RMSE on both the training and testing sets, and MAE on the testing set. Store these metrics for comparison.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8138ff5"
      },
      "source": [
        "**Reasoning**:\n",
        "To address the subtask, I will import the `Ridge` model, instantiate it with an alpha value, train it on the preprocessed data, make predictions, calculate and print the RMSE for both training and testing sets, and MAE for the testing set, and then store these metrics in the `model_performance` dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "422c29cf",
        "outputId": "9ab95886-2aed-453d-b097-93ec0254ac72"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# 1. Instantiate a Ridge Regression model with alpha=1.0\n",
        "ridge_reg_model = Ridge(alpha=1.0)\n",
        "\n",
        "# 2. Train the model\n",
        "ridge_reg_model.fit(X_train, y_train)\n",
        "print(\"Ridge Regression model trained successfully.\\n\")\n",
        "\n",
        "# 3. Make predictions on the training data\n",
        "y_train_pred_ridge = ridge_reg_model.predict(X_train)\n",
        "\n",
        "# 4. Make predictions on the testing data\n",
        "y_test_pred_ridge = ridge_reg_model.predict(X_test)\n",
        "\n",
        "# 5. Calculate RMSE for the training set\n",
        "rmse_train_ridge = np.sqrt(mean_squared_error(y_train, y_train_pred_ridge))\n",
        "\n",
        "# 6. Calculate RMSE for the testing set\n",
        "rmse_test_ridge = np.sqrt(mean_squared_error(y_test, y_test_pred_ridge))\n",
        "\n",
        "# 7. Calculate MAE for the testing set\n",
        "mae_test_ridge = mean_absolute_error(y_test, y_test_pred_ridge)\n",
        "\n",
        "# 8. Print the calculated metrics\n",
        "print(f\"Ridge Regression - Training RMSE: {rmse_train_ridge:.2f}\")\n",
        "print(f\"Ridge Regression - Testing RMSE: {rmse_test_ridge:.2f}\")\n",
        "print(f\"Ridge Regression - Testing MAE: {mae_test_ridge:.2f}\\n\")\n",
        "\n",
        "# 9. Store these metrics in the 'model_performance' dictionary\n",
        "model_performance['Ridge Regression'] = {\n",
        "    'Train RMSE': rmse_train_ridge,\n",
        "    'Test RMSE': rmse_test_ridge,\n",
        "    'Test MAE': mae_test_ridge\n",
        "}\n",
        "\n",
        "print(\"Metrics stored in 'model_performance' dictionary:\")\n",
        "print(model_performance)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge Regression model trained successfully.\n",
            "\n",
            "Ridge Regression - Training RMSE: 68435.25\n",
            "Ridge Regression - Testing RMSE: 70068.81\n",
            "Ridge Regression - Testing MAE: 50678.36\n",
            "\n",
            "Metrics stored in 'model_performance' dictionary:\n",
            "{'Linear Regression': {'Train RMSE': np.float64(68433.93736666226), 'Test RMSE': np.float64(70060.52184473521), 'Test MAE': 50670.73824097188}, 'Ridge Regression': {'Train RMSE': np.float64(68435.2525775098), 'Test RMSE': np.float64(70068.80893507139), 'Test MAE': 50678.35859391172}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b644c03"
      },
      "source": [
        "## Train and Evaluate Decision Tree Regressor Model\n",
        "\n",
        "### Subtask:\n",
        "Train a Decision Tree Regressor model on the preprocessed training data. Evaluate its performance using RMSE on both the training and testing sets, and MAE on the testing set. Store these metrics for comparison.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40746116"
      },
      "source": [
        "**Reasoning**:\n",
        "To address the subtask, I will import the `DecisionTreeRegressor` model, instantiate it, train it on the preprocessed data, make predictions on both training and testing sets, calculate and print the RMSE for both sets and MAE for the testing set, and then store these metrics in the `model_performance` dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f59ba941",
        "outputId": "53c3bde3-2e1f-4f42-ec43-775d1fcf0b73"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# 1. Instantiate a Decision Tree Regressor model\n",
        "decision_tree_reg_model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# 2. Train the model\n",
        "decision_tree_reg_model.fit(X_train, y_train)\n",
        "print(\"Decision Tree Regressor model trained successfully.\\n\")\n",
        "\n",
        "# 3. Make predictions on the training data\n",
        "y_train_pred_dt = decision_tree_reg_model.predict(X_train)\n",
        "\n",
        "# 4. Make predictions on the testing data\n",
        "y_test_pred_dt = decision_tree_reg_model.predict(X_test)\n",
        "\n",
        "# 5. Calculate RMSE for the training set\n",
        "rmse_train_dt = np.sqrt(mean_squared_error(y_train, y_train_pred_dt))\n",
        "\n",
        "# 6. Calculate RMSE for the testing set\n",
        "rmse_test_dt = np.sqrt(mean_squared_error(y_test, y_test_pred_dt))\n",
        "\n",
        "# 7. Calculate MAE for the testing set\n",
        "mae_test_dt = mean_absolute_error(y_test, y_test_pred_dt)\n",
        "\n",
        "# 8. Print the calculated metrics\n",
        "print(f\"Decision Tree Regressor - Training RMSE: {rmse_train_dt:.2f}\")\n",
        "print(f\"Decision Tree Regressor - Testing RMSE: {rmse_test_dt:.2f}\")\n",
        "print(f\"Decision Tree Regressor - Testing MAE: {mae_test_dt:.2f}\\n\")\n",
        "\n",
        "# 9. Store these metrics in the 'model_performance' dictionary\n",
        "model_performance['Decision Tree Regressor'] = {\n",
        "    'Train RMSE': rmse_train_dt,\n",
        "    'Test RMSE': rmse_test_dt,\n",
        "    'Test MAE': mae_test_dt\n",
        "}\n",
        "\n",
        "print(\"Metrics stored in 'model_performance' dictionary:\")\n",
        "print(model_performance)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor model trained successfully.\n",
            "\n",
            "Decision Tree Regressor - Training RMSE: 0.00\n",
            "Decision Tree Regressor - Testing RMSE: 69790.18\n",
            "Decision Tree Regressor - Testing MAE: 44214.67\n",
            "\n",
            "Metrics stored in 'model_performance' dictionary:\n",
            "{'Linear Regression': {'Train RMSE': np.float64(68433.93736666226), 'Test RMSE': np.float64(70060.52184473521), 'Test MAE': 50670.73824097188}, 'Ridge Regression': {'Train RMSE': np.float64(68435.2525775098), 'Test RMSE': np.float64(70068.80893507139), 'Test MAE': 50678.35859391172}, 'Decision Tree Regressor': {'Train RMSE': np.float64(0.0), 'Test RMSE': np.float64(69790.18312903232), 'Test MAE': 44214.67223837209}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d315cd0e"
      },
      "source": [
        "## Generate Model Comparison Table\n",
        "\n",
        "### Subtask:\n",
        "Create a comparison table summarizing the RMSE (Train), RMSE (Test), and MAE (Test) for all trained models (Linear Regression, Ridge Regression, and Decision Tree Regressor).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bee603e6"
      },
      "source": [
        "**Reasoning**:\n",
        "To fulfill the subtask, I need to convert the existing `model_performance` dictionary into a pandas DataFrame to create a structured comparison table and then display it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad8adda8",
        "outputId": "8720af07-c869-42d5-f62e-6c53aeac17bb"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the model_performance dictionary into a pandas DataFrame\n",
        "comparison_df = pd.DataFrame.from_dict(model_performance, orient='index')\n",
        "\n",
        "# Display the comparison table\n",
        "print(\"\\nModel Performance Comparison Table:\")\n",
        "print(comparison_df)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance Comparison Table:\n",
            "                           Train RMSE     Test RMSE      Test MAE\n",
            "Linear Regression        68433.937367  70060.521845  50670.738241\n",
            "Ridge Regression         68435.252578  70068.808935  50678.358594\n",
            "Decision Tree Regressor      0.000000  69790.183129  44214.672238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20e5b320"
      },
      "source": [
        "## Analyze Model Performance and Discuss ML Issues\n",
        "\n",
        "### Subtask:\n",
        "Provide a brief note explaining where underfitting (high bias) and overfitting (high variance) might have occurred across the different models based on their training and testing errors. Also, comment on at least one real-world machine learning issue observed (e.g., noisy features, outliers, non-linearity, or dataset bias) in the context of this dataset or models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1eb532f"
      },
      "source": [
        "### Model Performance Analysis and Real-World ML Issues\n",
        "\n",
        "**Underfitting and Overfitting Analysis:**\n",
        "\n",
        "*   **Linear Regression:**\n",
        "    *   `Train RMSE`: 68433.94\n",
        "    *   `Test RMSE`: 70060.52\n",
        "    *   Both training and testing RMSE values are relatively high and quite close to each other. This suggests that the Linear Regression model might be **underfitting** the data, meaning it's too simple to capture the underlying patterns. The model has high bias, as it performs similarly (and not excellently) on both seen and unseen data.\n",
        "\n",
        "*   **Ridge Regression:**\n",
        "    *   `Train RMSE`: 68435.25\n",
        "    *   `Test RMSE`: 70068.81\n",
        "    *   Similar to Linear Regression, Ridge Regression also shows high and close RMSE values for both training and testing sets. This indicates that Ridge Regression, despite regularization, also appears to be **underfitting** the dataset. The regularization did not significantly improve the model's ability to capture complexity, suggesting the core issue might be the model's simplicity for this data.\n",
        "\n",
        "*   **Decision Tree Regressor:**\n",
        "    *   `Train RMSE`: 0.00\n",
        "    *   `Test RMSE`: 69790.18\n",
        "    *   The Decision Tree Regressor exhibits a perfect `Train RMSE` of 0.00, which is a strong indicator of **overfitting**. The model has learned the training data perfectly, including noise, but its performance drops significantly on the unseen test data. This model has high variance, failing to generalize well.\n",
        "\n",
        "**Real-World Machine Learning Issue: Non-linearity and Outliers**\n",
        "\n",
        "One significant real-world machine learning issue observed in the context of this dataset and models is the presence of **non-linear relationships** within the data that linear models struggle to capture effectively, and potentially the impact of **outliers**.\n",
        "\n",
        "1.  **Non-linearity**: The similar and relatively high RMSEs for both Linear and Ridge Regression suggest that the relationship between the features (like `median_income`, `total_rooms`, `housing_median_age`, etc.) and the target variable (`median_house_value`) is likely not purely linear. Simple linear models cannot adequately represent the true complexity of the housing market data. This could be due to interaction effects between features or thresholds that affect housing prices in a non-linear fashion (e.g., beyond a certain income level, house prices might increase at a different rate).\n",
        "\n",
        "2.  **Outliers/Data Distribution**: While not explicitly analyzed in detail, the descriptive statistics from the initial inspection (e.g., large differences between mean and median for `total_rooms`, `total_bedrooms`, `population`, `households`, and the max values being significantly higher than 75th percentile) hint at the presence of outliers or skewed distributions in several numerical features. These outliers can heavily influence linear models by pulling the regression line towards them, leading to higher overall error rates. For decision trees, extreme outliers might be captured as specific nodes if the tree grows deep enough, contributing to the overfitting observed.\n",
        "\n",
        "To address these issues, more complex models (like Random Forests or Gradient Boosting) or techniques such as feature engineering to create non-linear features, robust scaling methods, or outlier detection and handling might be necessary. The task already hints at a complex relationship between features and target, and the simple models confirm this complexity is not fully captured."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be3748c6"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the regression analysis, including the performance of different models and key insights regarding model complexity and generalization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5698ee8"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Dataset Overview**: The `housing.csv` dataset contains 20,640 entries and 10 columns. `total_bedrooms` had 207 missing values, which were imputed using the median. `ocean_proximity` was identified as a categorical feature.\n",
        "*   **Data Preprocessing**:\n",
        "    *   Missing values in `total_bedrooms` were filled with the median.\n",
        "    *   The `ocean_proximity` categorical feature was one-hot encoded.\n",
        "    *   Numerical features were scaled using `StandardScaler`.\n",
        "    *   The data was split into training (16,512 samples) and testing (4,128 samples) sets.\n",
        "*   **Model Performance Comparison**:\n",
        "    *   **Linear Regression**: Showed a Training RMSE of \\$68,433.94\\$, Testing RMSE of \\$70,060.52\\$, and Testing MAE of \\$50,670.74\\$. Both RMSEs were high and close, indicating potential underfitting.\n",
        "    *   **Ridge Regression**: Exhibited similar performance with a Training RMSE of \\$68,435.25\\$, Testing RMSE of \\$70,068.81\\$, and Testing MAE of \\$50,678.36\\$. This also suggested underfitting, as regularization did not significantly alter performance compared to standard Linear Regression.\n",
        "    *   **Decision Tree Regressor**: Achieved a Training RMSE of \\$0.00\\$ but a Testing RMSE of \\$69,790.18\\$ and Testing MAE of \\$44,214.67\\$. The perfect training score with a much higher test score is a clear sign of severe overfitting.\n",
        "*   **Model Complexity and Generalization**:\n",
        "    *   Linear and Ridge Regression models appear to be **underfitting** the data. Their high and similar training and testing error rates suggest they are too simple to capture the complex relationships within the housing data.\n",
        "    *   The Decision Tree Regressor severely **overfitted** the training data, as evidenced by its \\$0.00\\$ training RMSE, but failed to generalize well to unseen data, resulting in a high testing RMSE.\n",
        "*   **Real-World Machine Learning Issues**: The analysis revealed two key issues:\n",
        "    *   **Non-linearity**: The poor performance of linear models suggests that the relationship between features and housing prices is likely non-linear, which simple linear models cannot adequately capture.\n",
        "    *   **Outliers/Data Distribution**: Initial inspection indicated skewed distributions and potential outliers in several numerical features, which can negatively impact model training, especially for linear models and unconstrained decision trees.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The dataset exhibits non-linear relationships and potentially outliers, making simple linear models prone to underfitting and unconstrained decision trees prone to overfitting. More sophisticated models or feature engineering are necessary to improve predictive performance.\n",
        "*   Future steps should include exploring more robust regression models (e.g., Random Forest, Gradient Boosting), hyperparameter tuning for the Decision Tree to mitigate overfitting (e.g., setting `max_depth`), and potentially advanced outlier detection and handling techniques.\n"
      ]
    }
  ]
}